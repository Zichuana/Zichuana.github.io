---
layout:     post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title:      pFindå¯¼å‡ºæ–‡ä»¶æ•°æ®å¤„ç†              # æ ‡é¢˜ 
subtitle:   ç®€å•å®ç”¨çš„è›‹ç™½è´¨ç»„å­¦æ•°æ®å¤„ç†è„šæœ¬      # å‰¯æ ‡é¢˜
date:       2025-9-6              # æ—¶é—´
author:     zichuana                     # ä½œè€…
header-img: img/2025-9-1/cover.png   #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
    - ç”Ÿä¿¡
    - è›‹ç™½è´¨ç»„å­¦
---
### æå–OXæˆ–è€…OS
```python

import pandas as pd
import requests
import re

files = {
    "Archaea": "annotated_Archaea.xlsx",
    "Bacteria": "annotated_Bacteria.xlsx",
    "Fungi": "annotated_Fungi.xlsx"
}

for group, file in files.items():
    # 1. è¯»å–å½“å‰æ–‡ä»¶
    df = pd.read_excel(file)

    # 2. ä» DE æå– OX
    def extract_ox(de):
        m = re.search(r'OX=(\d+)', str(de))
        return int(m.group(1)) if m else None

    df['OX'] = df['DE'].apply(extract_ox)

    # 5. ä¿å­˜æ–°æ–‡ä»¶
    df.to_excel("annotated_" + group + "_OX.xlsx", index=False)
    print(f"âœ… å·²ç”Ÿæˆ annotated_{group}_OX.xlsx")
```

```python
import pandas as pd

# è¯»å– Excel æ–‡ä»¶ä¸­çš„æ‰€æœ‰å·¥ä½œè¡¨
xls = pd.read_excel(r'e:\\Protein\result\\9-4Archiving\\Summry_archaea_bacteria_fungi.xlsx', sheet_name=None)
print(xls)

def extract_species(de):
    if pd.isna(de):
        return None
    if "OS=" in str(de) and "OX=" in str(de):
        return str(de).split("OS=")[1].split(" OX=")[0]
    return None

# å¤„ç†æ¯ä¸ªå·¥ä½œè¡¨
for sheet_name, df in xls.items():
    df["Species"] = df["DE"].apply(extract_species)
    output_file = f"annotated_{sheet_name}.xlsx"
    df.to_excel(output_file, index=False)
    print(f"âœ… å·²ç”Ÿæˆï¼š{output_file}")

if False:
    # åˆå¹¶æ‰€æœ‰å·¥ä½œè¡¨ä¸ºä¸€ä¸ªæ€»è¡¨
    combined = pd.concat(xls.values(), keys=xls.keys())
    combined.reset_index(level=0, inplace=True)
    combined.rename(columns={'level_0': 'Sheet'}, inplace=True)
    combined.to_excel("annotated_combined.xlsx", index=False)
```

### æ ¹æ®oxä»ncbiä¸­è·å–çº²ç›®ç§‘å±ç³»
```python
import pandas as pd
from ete3 import NCBITaxa

# è¯»å– Excel æ–‡ä»¶
df = pd.read_excel('annotated_Fungi_OX.xlsx', sheet_name='Sheet1')

# ç¡®ä¿ OX åˆ—ä¸ºæ•´æ•°ç±»å‹
df['OX'] = pd.to_numeric(df['OX'], errors='coerce').astype('Int64')

# åˆå§‹åŒ– NCBI taxonomy
ncbi = NCBITaxa()

# å®šä¹‰è¦æå–çš„ç­‰çº§
ranks = ['class', 'genus', 'order', 'family']

# ç”¨äºå­˜å‚¨ order å’Œ family çš„åˆ—è¡¨
order_list = []
family_list = []
class_list = []
genus_list = []

for taxid in df['OX'].dropna():
    try:
        lineage = ncbi.get_lineage(taxid)
        names = ncbi.get_taxid_translator(lineage)
        rank_dict = ncbi.get_rank(lineage)
        result = {rank: names[tid] for tid, rank in rank_dict.items() if rank in ranks}
        order_list.append(result.get('order', None))
        family_list.append(result.get('family', None))
        genus_list.append(result.get('genus', None))
        class_list.append(result.get('class', None))
    except Exception as e:
        order_list.append(None)
        family_list.append(None)
        class_list.append(None)
        genus_list.append(None)

# æ·»åŠ æ–°åˆ—åˆ°åŸ DataFrame
df['order'] = order_list
df['family'] = family_list
df['class'] = class_list
df['genus'] = genus_list    

# è¦†ç›–ä¿å­˜å›åŸ Excel æ–‡ä»¶
df.to_excel('annotated_Fungi_OX.xlsx', sheet_name='Sheet1', index=False)

print("âœ… å®Œæˆï¼Œå·²ç›´æ¥åœ¨åŸ Excel æ–‡ä»¶ä¸­æ·»åŠ åˆ—ã€‚")
```

### é€šè¿‡UniProt IDè½¬kegg
```python
# ========================= 0. ä¾èµ– =========================
import pandas as pd
import requests
from tqdm import tqdm
import re
import time
from io import StringIO

# ========================= 1. è¯» Excel =========================
file_name = "Summry_archaea_bacteria_fungi.xlsx"   # åŸå§‹æ–‡ä»¶ï¼ŒåŒ…å« 3 ä¸ª sheetï¼ˆArchaea / Bacteria / Fungiï¼‰

# ========================= 2. ä» AC åˆ—æŠ½ UniProt ID =========================
def extract_uniprot_id(ac_str):
    """
    AC åˆ—å…¸å‹æ ¼å¼ï¼šsp|P12345|PROTEIN_NAME
    æˆ‘ä»¬åªè¦ç¬¬äºŒæ®µ |P12345| è¿™ä¸€æ®µä½œä¸º UniProt ACã€‚
    å¦‚æœä¸ºç©ºæˆ–æ ¼å¼ä¸å¯¹ï¼Œè¿”å› Noneã€‚
    """
    if pd.isna(ac_str):
        return None
    parts = str(ac_str).strip().split('|')
    if len(parts) >= 2:
        return parts[1]
    return None

# ========================= 3. UniProt å®˜æ–¹ ID mapping API =========================
UNIPROT_MAP_URL = "https://rest.uniprot.org/idmapping"   # 2022 æ–°ç‰ˆ REST ç«¯ç‚¹

def uniprot_map(ids, from_db="UniProtKB_AC-ID", to_db="KEGG", chunk_size=5000):
    """
    æŠŠä¸€é•¿ä¸² UniProt AC åˆ†æ‰¹æ˜ å°„åˆ°ç›®æ ‡æ•°æ®åº“ï¼ˆKEGG/COG/ECï¼‰ã€‚
    æ¯æ‰¹ â‰¤5000 æ˜¯å®˜æ–¹å»ºè®®ä¸Šé™ï¼Œæœ€åå†æ‹¼æˆä¸€ä¸ªå¤§ DataFrameã€‚
    """
    # 3.1 åˆ†æ‰¹
    chunks = [ids[i:i+chunk_size] for i in range(0, len(ids), chunk_size)]
    parts = []
    for c in chunks:
        parts.append(_uniprot_map_one_chunk(c, from_db, to_db))
    return pd.concat(parts, ignore_index=True)


def _uniprot_map_one_chunk(ids, from_db, to_db):
    print(f"[{time.strftime('%X')}] æäº¤ {len(ids)} æ¡ -> {to_db}")
    r1 = requests.post(
        f"{UNIPROT_MAP_URL}/run",
        data={"from": from_db, "to": to_db, "ids": ",".join(ids)},
        timeout=30
    )
    print(f"[{time.strftime('%X')}] æäº¤è¿”å›ç  {r1.status_code}")
    if r1.status_code != 200:
        print(r1.text[:200])
        return pd.DataFrame(columns=["UniprotID", to_db])

    job_id = r1.json().get("jobId")
    print(f"[{time.strftime('%X')}] jobId {job_id}")
    if not job_id:
        return pd.DataFrame(columns=["UniprotID", to_db])

    # --- è½®è¯¢ä¸Šé™ 120 æ¬¡ï¼ˆçº¦ 3 åˆ†é’Ÿï¼‰ï¼Œé˜²æ­¢æ­»å¾ªç¯ ---
    for poll in range(120):
        r2 = requests.get(f"{UNIPROT_MAP_URL}/status/{job_id}", timeout=30)
        print(f"[DEBUG] è½®è¯¢ {poll:02d} è¿”å›é•¿åº¦ {len(r2.text)} å­—ç¬¦")

        # ç­–ç•¥ 1ï¼šå‡ºç° results ä¸”é•¿åº¦åˆç† â†’ å®Œæˆ
        if r2.status_code == 200 and "results" in r2.text and len(r2.text) > 50:
            print(f"[DEBUG] æ£€æµ‹åˆ° resultsï¼Œjob å·²å®Œæˆ")
            job_status = "FINISHED"
            break

        # ç­–ç•¥ 2ï¼šè¿”å›é•¿åº¦å¾ˆå°ä¸”æ—  jobStatus â†’ ç©ºç»“æœï¼Œä¹Ÿè§†ä¸ºå®Œæˆ
        if poll == 0:
            first_len = len(r2.text)
        if len(r2.text) == first_len and poll > 3:
            print(f"[DEBUG] è¿ç»­å¤šæ¬¡ç›¸åŒçŸ­å“åº”ï¼Œè§†ä¸ºç©ºç»“æœï¼Œè·³å‡º")
            job_status = "FINISHED"
            break

        # ç­–ç•¥ 3ï¼šæ­£å¸¸å­—æ®µåˆ¤æ–­
        if r2.headers.get("content-type", "").startswith("application/json"):
            job_status = r2.json().get("jobStatus")
            if job_status == "FINISHED":
                break
            if job_status in {"ERROR", "FAILED"}:
                print("[ERROR] Job failed:", r2.text[:200])
                return pd.DataFrame(columns=["UniprotID", to_db])
        time.sleep(1.5)
    else:
        print("[ERROR] è½®è¯¢ 120 æ¬¡ä»æœªå®Œæˆï¼Œå¼ºåˆ¶æ”¾å¼ƒ")
        return pd.DataFrame(columns=["UniprotID", to_db])

    # ---------------- å…³é”®ï¼šæŠŠç»“æœè§£ææˆ DataFrame ----------------
    results_url = f"{UNIPROT_MAP_URL}/results/{job_id}"
    r3 = requests.get(results_url, timeout=30)
    if r3.status_code != 200:
        print("[ERROR] è·å– results å¤±è´¥:", r3.text[:200])
        return pd.DataFrame(columns=["UniprotID", to_db])

    data = r3.json()
    if not data or "results" not in data or not data["results"]:
        # ç©ºæ˜ å°„
        return pd.DataFrame(columns=["UniprotID", to_db])

    # æ•´ç†æˆä¸¤åˆ—ï¼šfrom -> to
    records = []
    for item in data["results"]:
        # from æ˜¯ UniProtACï¼Œto æ˜¯ç›®æ ‡åº“ ID
        records.append({
            "UniprotID": item["from"],
            to_db: item["to"]
        })
    return pd.DataFrame(records)
    
# ========================= 4. å¤„ç†å•ä¸ª kingdom =========================
def process_kingdom(sheet_name):
    """
    å¯¹å•ä¸ª sheetï¼ˆArchaea/Bacteria/Fungiï¼‰ï¼š
    1) è¯»è¡¨ â†’ æŠ½ UniProt ID
    2) åˆ†åˆ«æ˜ å°„ KEGG(KO)ã€COGã€EC å·
    3) åˆå¹¶å›åŸè¡¨æ³¨é‡Šåˆ—
    4) è¾“å‡º csv
    """
    print(f"\nğŸ“Š å¤„ç† {sheet_name}...")
    df = pd.read_excel(file_name, sheet_name=sheet_name)
    df['UniprotID'] = df['AC'].apply(extract_uniprot_id)
    ids = df['UniprotID'].dropna().unique().tolist()
    print(f"å…± {len(ids)} ä¸ª UniProt ID")

    # 4.1 ä¸‰æ¬¡æ˜ å°„
    ko  = uniprot_map(ids, "UniProtKB_AC-ID", "KEGG")   # KO å·
    # cog = uniprot_map(ids, "UniProtKB_AC-ID", "COG")    # COG ç±»
    # ec  = uniprot_map(ids, "UniProtKB_AC-ID", "Enzyme") # EC å·

    # 4.2 æŠŠæ³¨é‡Šåˆ—ï¼ˆDEã€PSM Countâ€¦ï¼‰å…ˆå»é‡ï¼Œå†è·Ÿä¸‰å¼ æ˜ å°„è¡¨ left join
    annot = df[['UniprotID', 'DE', 'PSM Count', 'Coverage', 'Score']].drop_duplicates(subset=['UniprotID'])
    annot = annot.merge(ko,  on='UniprotID', how='left')
    # annot = annot.merge(cog, on='UniprotID', how='left')
    # annot = annot.merge(ec,  on='UniprotID', how='left')

    # 4.3 ä¿å­˜
    output = f"{sheet_name.lower()}_annot.csv"
    annot.to_csv(output, index=False)
    print(f"âœ… å·²ä¿å­˜ï¼š{output}")
    return annot

# ========================= 5. ä¸»æµç¨‹ =========================
for kingdom in ["Bacteria", "Archaea", "Fungi"]:
    process_kingdom(kingdom)
```

### ç»“è®º
é’ˆå¯¹åœŸå£¤è›‹ç™½è´¨ç»„å­¦ä¸é€‚åˆåšåŠŸèƒ½å¯Œé›†ç­‰ä¸‹æ¸¸åˆ†æï¼Œç»“åˆåŸºå› ç»„å­¦å¯èƒ½ä¼šæœ‰æ„ä¹‰ï¼Œé€šè¿‡ https://www.uniprot.org/id-mapping/ è½¬æ¢æˆeggnog-mapperåœ¨å…¶ä»–æ•°æ®ä¸­å¯èƒ½ä¼šæœ‰æ„ä¹‰ï¼Œhttps://zhuanlan.zhihu.com/p/113015245