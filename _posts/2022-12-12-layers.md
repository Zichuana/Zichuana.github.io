---
layout:     post                    # 使用的布局（不需要改）
title:      卷积层、池化层、全连接层               # 标题 
subtitle:   深度学习经典三层笔记 #副标题
date:       2022-12-12              # 时间
author:     zichuana                     # 作者
header-img: img/2022-12-12/page.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 深度学习
---

### 卷积层
![image](/img/2022-12-12/a.png)  
假设当输入图像为`7*7*3`，3是它的深度（即R、G、B三色通道）卷积层是一个`5*5*3`的filter，这里注意：filter的深度必须和输入图像的深度相同。通过一个filter与输入图像的卷积可以得到一个`3*3*1`的特征图。  
当有两个卷积核进行卷积运算后，可以得到两个`3*3*1`的特征图（feature_map）。注意：这两个卷积核可以不一致，以获得不相同的特征。  
![image](/img/2022-12-12/b.png)  
中间转化过程距离，卷积核滑块在input上移动获取特征信息。  
以`input`为`3*3*1`，`filter`为`2*2*1`，`output`为`2*2*1`为例。  
![image](/img/2022-12-12/c.png)  
计算第一个卷积层神经元o11的输入:  
`neto11=conv(input,filter)=i11×f11+i12×f12+i21×f21+i22×f22=1×1+0×2+0×4+1×5=6`  

神经元o11的输出:(此处使用Relu激活函数)  
`outo11=activators(neto11)=max(0, neto11)=6`  
滑块移动展示：  
![image](/img/2022-12-12/d.gif)
动图来自[https://blog.csdn.net/silence1214/article/details/11809947](https://blog.csdn.net/silence1214/article/details/11809947)  
参考[https://zhuanlan.zhihu.com/p/59917842](https://zhuanlan.zhihu.com/p/59917842)
### 池化层
![image](/img/2022-12-12/f.png)
池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。  
![image](/img/2022-12-12/e.png)
常用池化方法中随机池化如上图所示，计算过程：
- 先将方格①中的元素同时除以它们的和sum（sum=1+3+4+2=10），得到概率矩阵②
- 按照概率随机选中方格③
- pooling得到的值就是方格位置的值，这里为4

组合池化是同时利用最大值池化与均值池化两种的优势而引申的一种池化策略，包括CAT和ADD。其它的常用池化方法都是字面意思。  
代码可以参考[https://zhuanlan.zhihu.com/p/77040467](https://zhuanlan.zhihu.com/p/77040467)
### 全连接层
可以用四个字概括：**特征加权**    
在实际使用中，全连接层的`input`和`output`之间还是在中间做卷积。  
举例：将`a*b*c*100`转化成`1*1*100*1`  
![image](/img/2022-12-12/g.png)  
用一个`a*b*c`的filter去卷积激活函数的输出，得到的结果就是一个`fully connected layer`的一个神经元的输出，这个输出就是一个值。因为有100个神经元。实际就是用一个`a*b*c*100`的卷积层去卷积激活函数的输出。通过100个特征对结果进行10分类。  
全连接的核心操作就是矩阵向量乘积。